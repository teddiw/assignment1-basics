{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80777d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7038d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "seq_len = 1024\n",
    "num_heads = 25\n",
    "d_k = d_model // num_heads\n",
    "d_ff = 6400 # normallyd_model * 8/3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ee68af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\noindent Total FLOPs: 4.533469e+12 \\\\\n",
      "\n",
      "\\noindent Proportion of FLOPs in Attention Blocks: 96.37\\% \\\\\n",
      "\\noindent Proportion of FLOPs for Output Embedding: 3.63\\% \\\\\n",
      "\n",
      "\\noindent Proportion of FLOPs for RoPE: 0.46\\% \\\\\n",
      "\\noindent Proportion of FLOPs for Multi-Head Attention with RoPE: 30.88\\% \\\\\n",
      "\\noindent Proportion of FLOPs for Position-Wise Feed-Forward: 69.12\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "num_matmul_flops = 0 \n",
    "\n",
    "attn_block_flops = 0\n",
    "rope_flops = 0\n",
    "mha_rope_flops = 0\n",
    "pw_feed_forward_flops = 0\n",
    "\n",
    "output_embedding_flops = 0\n",
    "# Embedding\n",
    "num_matmul_flops += 0\n",
    "\n",
    "for i in range(num_layers):\n",
    "    checkpoint1_num_matmul_flops = num_matmul_flops\n",
    "    ###### START Tranformer block ######\n",
    "\n",
    "    # RMSNorm 1\n",
    "    num_matmul_flops += 0\n",
    "\n",
    "    # MHA with RoPE\n",
    "    checkpoint2_num_matmul_flops = num_matmul_flops\n",
    "    # 3 matric multiplies for Q, K, V: \"... batch seq d_model, h_d_k d_model -> ... batch seq h_d_k\"\n",
    "    num_matmul_flops += 3 * (2 * seq_len * d_model * (num_heads*d_k))\n",
    "\n",
    "    # RoPE for Q and K for num_heads: \"... seq d_k1 d_k2, ... seq d_k2 -> ... seq d_k1\"\n",
    "    num_matmul_flops += num_heads * (2 * (2 * d_k * d_k * seq_len))\n",
    "    rope_flops += num_heads * (2 * (2 * d_k * d_k * seq_len))\n",
    "\n",
    "    # scaled_dot_product_attention 1 for num_heads: \"... seq d_k, ... seq d_k -> ... seq seq\"\n",
    "    num_matmul_flops += num_heads * (2 * seq_len * d_k * seq_len)\n",
    "\n",
    "    # scaled_dot_product_attention 2 for num_heads: \"... seq seq, ... seq d_k -> ... seq d_k\"\n",
    "    num_matmul_flops += num_heads * (2 * seq_len * seq_len * d_k)\n",
    "\n",
    "    # MHA output projection: \"... batch seq h_d_k, d_model h_d_k-> ... batch seq d_model\"\n",
    "    num_matmul_flops += 2 * seq_len * (num_heads*d_k) * d_model\n",
    "\n",
    "    mha_rope_flops += num_matmul_flops - checkpoint2_num_matmul_flops\n",
    "\n",
    "    # RMSNorm 2\n",
    "    num_matmul_flops += 0\n",
    "\n",
    "    # PositionwiseFeedforward: \n",
    "    # \"batch seq d_model, d_ff d_model -> batch seq d_ff\"\n",
    "    # \"batch seq d_model, d_ff  d_model -> batch seq d_ff\"\n",
    "    # \"batch seq d_ff, d_model  d_ff -> batch seq d_model\"\n",
    "    num_matmul_flops += 3 * (2 * seq_len * d_model * d_ff)\n",
    "    pw_feed_forward_flops += 3 * (2 * seq_len * d_model * d_ff)\n",
    "\n",
    "    ###### END Tranformer block ######\n",
    "    attn_block_flops += num_matmul_flops - checkpoint1_num_matmul_flops\n",
    "    \n",
    "# Last RMSNorm \"batch seq d_ff -> batch seq d_ff\"\n",
    "num_matmul_flops += 0\n",
    "\n",
    "# Linear projection: \"batch seq d_ff, vocab_size d_ff-> batch seq vocab_size\"\n",
    "num_matmul_flops += 2 * seq_len * d_model * vocab_size\n",
    "output_embedding_flops += 2 * seq_len * d_model * vocab_size\n",
    "# Softmax\n",
    "num_matmul_flops += 0\n",
    "\n",
    "print(f'\\\\noindent Total FLOPs: {\"{:e}\".format(num_matmul_flops).format()} \\\\\\\\')\n",
    "print()\n",
    "print(f'\\\\noindent Proportion of FLOPs in Attention Blocks: {round(100*attn_block_flops/num_matmul_flops, 2)}\\\\% \\\\\\\\')\n",
    "print(f'\\\\noindent Proportion of FLOPs for Output Embedding: {round(100*output_embedding_flops/num_matmul_flops, 2)}\\\\% \\\\\\\\')\n",
    "print()\n",
    "print(f'\\\\noindent Proportion of FLOPs for RoPE: {round(100*rope_flops/attn_block_flops, 2)}\\\\% \\\\\\\\')\n",
    "print(f'\\\\noindent Proportion of FLOPs for Multi-Head Attention with RoPE: {round(100*mha_rope_flops/attn_block_flops, 2)}\\\\% \\\\\\\\')\n",
    "print(f'\\\\noindent Proportion of FLOPs for Position-Wise Feed-Forward: {round(100*pw_feed_forward_flops/attn_block_flops, 2)}\\\\% \\\\\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc996c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.49844918272"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "149844918272000/1e14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ef0c7",
   "metadata": {},
   "source": [
    "## Count trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a7bbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\noindent Total trainable parameters: 2.046646e+09 \\\\\n"
     ]
    }
   ],
   "source": [
    "num_parameters = 0\n",
    "# Embedding\n",
    "num_parameters += 0\n",
    "\n",
    "for i in range(num_layers):\n",
    "    ###### START Tranformer block ######\n",
    "\n",
    "    # RMSNorm 1\n",
    "    num_parameters += d_model\n",
    "\n",
    "    # MHA with RoPE\n",
    "    # 3 matric multiplies for Q, K, V: \"... batch seq d_model, h_d_k d_model -> ... batch seq h_d_k\"\n",
    "    num_parameters += num_heads * (d_model / num_heads) * d_model\n",
    "    num_parameters += num_heads * (d_model / num_heads) * d_model\n",
    "    num_parameters += num_heads * (d_model / num_heads) * d_model\n",
    "\n",
    "    # RoPE for Q and K for num_heads: \"... seq d_k1 d_k2, ... seq d_k2 -> ... seq d_k1\"\n",
    "    num_parameters += 0\n",
    "\n",
    "    # MHA output projection: \"... batch seq h_d_k, d_model h_d_k-> ... batch seq d_model\"\n",
    "    num_parameters += (d_model / num_heads) * num_heads * d_model\n",
    "\n",
    "    # RMSNorm 2\n",
    "    num_parameters += d_model\n",
    "\n",
    "    # PositionwiseFeedforward: \n",
    "    # \"batch seq d_model, d_ff d_model -> batch seq d_ff\"\n",
    "    # \"batch seq d_model, d_ff  d_model -> batch seq d_ff\"\n",
    "    # \"batch seq d_ff, d_model  d_ff -> batch seq d_model\"\n",
    "    num_parameters += d_model * d_ff\n",
    "    num_parameters += d_ff * d_model\n",
    "    num_parameters += d_model * d_ff\n",
    "\n",
    "    ###### END Tranformer block ######\n",
    "    \n",
    "# Last RMSNorm \"batch seq d_model -> batch seq d_model\"\n",
    "num_parameters += d_model\n",
    "\n",
    "# Linear projection: \"batch seq d_model, vocab_size d_model-> batch seq vocab_size\"\n",
    "num_parameters += vocab_size * d_model\n",
    "\n",
    "# Softmax\n",
    "num_parameters += 0\n",
    "print(f'\\\\noindent Total trainable parameters: {\"{:e}\".format(num_parameters).format()} \\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f61e2",
   "metadata": {},
   "source": [
    "# AdamW Optimizer Accounting\n",
    "\n",
    "The number of bytes needed to store the parameters, activations, gradients, and optimizer state is:\n",
    "4 * (4*num_parameters + num_activations)\n",
    "\n",
    "num_parameters and num_activations are defined below.\n",
    "\n",
    "\n",
    "\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce205cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "seq_len = 1024\n",
    "num_heads = 25\n",
    "d_k = d_model // num_heads\n",
    "d_ff = 6400 # normally d_model * 8/3 \n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb1487ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\noindent Total parameters: 2.284344e+09 \\\\\n"
     ]
    }
   ],
   "source": [
    "num_parameters = 0\n",
    "# Embedding\n",
    "num_parameters += vocab_size * d_model\n",
    "\n",
    "for i in range(num_layers):\n",
    "    ###### START Tranformer block ######\n",
    "\n",
    "    # RMSNorm 1\n",
    "    num_parameters += d_model\n",
    "\n",
    "    # MHA with RoPE\n",
    "    # 3 matric multiplies for Q, K, V: \"... batch seq d_model, h_d_k d_model -> ... batch seq h_d_k\"\n",
    "    num_parameters += d_model * d_model\n",
    "    num_parameters += d_model * d_model\n",
    "    num_parameters += d_model * d_model\n",
    "\n",
    "    # RoPE for Q and K for num_heads: \"... seq d_k1 d_k2, ... seq d_k2 -> ... seq d_k1\"\n",
    "    num_parameters += d_model/2 * seq_len * 4\n",
    "\n",
    "    # MHA output projection: \"... batch seq h_d_k, d_model h_d_k-> ... batch seq d_model\"\n",
    "    num_parameters += d_model * d_model\n",
    "\n",
    "    # RMSNorm 2\n",
    "    num_parameters += d_model\n",
    "\n",
    "    # PositionwiseFeedforward: \n",
    "    # \"batch seq d_model, d_ff d_model -> batch seq d_ff\"\n",
    "    # \"batch seq d_model, d_ff  d_model -> batch seq d_ff\"\n",
    "    # \"batch seq d_ff, d_model  d_ff -> batch seq d_model\"\n",
    "    num_parameters += d_model * d_ff\n",
    "    num_parameters += d_ff * d_model\n",
    "    num_parameters += d_model * d_ff\n",
    "\n",
    "    ###### END Tranformer block ######\n",
    "    \n",
    "# Last RMSNorm \"batch seq d_model -> batch seq d_model\"\n",
    "num_parameters += d_model\n",
    "\n",
    "# Linear projection: \"batch seq d_model, vocab_size d_model-> batch seq vocab_size\"\n",
    "num_parameters += vocab_size * d_model\n",
    "\n",
    "# Softmax\n",
    "num_parameters += 0\n",
    "print(f'\\\\noindent Total parameters: {\"{:e}\".format(num_parameters).format()} \\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b4cd9c",
   "metadata": {},
   "source": [
    "### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29ed57c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\noindent total number of activations: 1.159266e+10 \\\\\n"
     ]
    }
   ],
   "source": [
    "num_activations = 0\n",
    "# Embedding\n",
    "# Float[Tensor, \"batch seq d_model\"]\n",
    "# num_activations += batch_size * seq_len * d_model\n",
    "\n",
    "for i in range(num_layers):\n",
    "    ###### START Tranformer block ######\n",
    "    # RMSNorm 1\n",
    "    # Float[Tensor, \"batch seq d_model\"]\n",
    "    num_activations += batch_size * seq_len * d_model\n",
    "\n",
    "    # MHA \n",
    "    # 3 matrix multiplies for Q, K, V projections: \"... batch seq d_model, h_d_k d_model -> ... batch seq h_d_k\"\n",
    "    num_activations += batch_size * seq_len * d_model\n",
    "\n",
    "    # Q.T @ K\n",
    "    # einsum(Q, K, \"... n_queries d_k, ... m_keys d_k -> ... n_queries m_keys\")\n",
    "    num_activations += seq_len * seq_len\n",
    "\n",
    "    # Softmax \n",
    "    num_activations += seq_len * seq_len\n",
    "\n",
    "    # @ V\n",
    "    # einsum(temp2, V, \"... n_queries m_keys, ... m_keys d_v -> ... n_queries d_v\")\n",
    "    num_activations += seq_len * d_model\n",
    "\n",
    "    # MHA output projection: \n",
    "    # einsum(result, self.W_o, \"... batch seq h_d_k, d_model h_d_k-> ... batch seq d_model\")\n",
    "    num_activations += batch_size * seq_len * d_model\n",
    "\n",
    "    # RMSNorm 2\n",
    "    # Float[Tensor, \"batch seq d_model\"]\n",
    "    num_activations += batch_size * seq_len * d_model\n",
    "\n",
    "    # PositionwiseFeedforward W_1 for num_heads: \"... seq d_k, ... seq d_k -> ... seq seq\"\n",
    "    # einsum(x, self.w1, \"batch seq d_model, d_ff d_model -> batch seq d_ff\")\n",
    "    num_activations += batch_size * seq_len * 4*d_model\n",
    "\n",
    "    # PositionwiseFeedforward sigmoid\n",
    "    # einsum(temp4, self.w2, \"batch seq d_ff, d_model  d_ff -> batch seq d_model\")\n",
    "    num_activations += batch_size * seq_len * d_model\n",
    "\n",
    "    # # PositionwiseFeedforward W_2 for num_heads: \"... seq seq, ... seq d_k -> ... seq d_k\"\n",
    "    # num_activations += batch_size * seq_len * d_ff\n",
    "\n",
    "    ###### END Tranformer block ######\n",
    "    \n",
    "# Last RMSNorm \"batch seq d_ff -> batch seq d_ff\"\n",
    "# Float[Tensor, \"batch seq d_model\"]\n",
    "num_activations += batch_size * seq_len * d_model\n",
    "\n",
    "# Linear projection: \"batch seq d_ff, vocab_size d_ff-> batch seq vocab_size\"\n",
    "num_activations += batch_size * seq_len * vocab_size\n",
    "\n",
    "# # Softmax\n",
    "# num_activations += \n",
    "\n",
    "# Cross-entropy\n",
    "# losses calculated by batch\n",
    "num_activations += batch_size \n",
    "\n",
    "print(f'\\\\noindent total number of activations: {\"{:e}\".format(num_activations).format()} \\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65623648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.22540956363082"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 * (4*num_parameters + num_activations) / 2**30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f873a",
   "metadata": {},
   "source": [
    "(c) There are no matmuls for an AdamW step. The same 12 elementwise operations are applied to each trainable parameter in each step; the number of FLOPs per AdamW step is: num_trainable_parameters*12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf71e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attrib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
