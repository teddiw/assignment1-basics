{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80777d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7038d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "seq_len = 1024\n",
    "num_heads = 25\n",
    "d_k = d_model // num_heads\n",
    "d_ff = 6400 # normallyd_model * 8/3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87ee68af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\noindent Total FLOPs: 4.533469e+12 \\\\\n",
      "\n",
      "\\noindent Proportion of FLOPs in Attention Blocks: 96.37\\% \\\\\n",
      "\\noindent Proportion of FLOPs for Output Embedding: 3.63\\% \\\\\n",
      "\n",
      "\\noindent Proportion of FLOPs for RoPE: 0.46\\% \\\\\n",
      "\\noindent Proportion of FLOPs for Multi-Head Attention with RoPE: 30.88\\% \\\\\n",
      "\\noindent Proportion of FLOPs for Position-Wise Feed-Forward: 69.12\\% \\\\\n"
     ]
    }
   ],
   "source": [
    "num_matmul_flops = 0 \n",
    "\n",
    "attn_block_flops = 0\n",
    "rope_flops = 0\n",
    "mha_rope_flops = 0\n",
    "pw_feed_forward_flops = 0\n",
    "\n",
    "output_embedding_flops = 0\n",
    "# Embedding\n",
    "num_matmul_flops += 0\n",
    "\n",
    "for i in range(num_layers):\n",
    "    checkpoint1_num_matmul_flops = num_matmul_flops\n",
    "    ###### START Tranformer block ######\n",
    "\n",
    "    # RMSNorm 1\n",
    "    num_matmul_flops += 0\n",
    "\n",
    "    # MHA with RoPE\n",
    "    checkpoint2_num_matmul_flops = num_matmul_flops\n",
    "    # 3 matric multiplies for Q, K, V: \"... batch seq d_model, h_d_k d_model -> ... batch seq h_d_k\"\n",
    "    num_matmul_flops += 3 * (2 * seq_len * d_model * (num_heads*d_k))\n",
    "\n",
    "    # RoPE for Q and K for num_heads: \"... seq d_k1 d_k2, ... seq d_k2 -> ... seq d_k1\"\n",
    "    num_matmul_flops += num_heads * (2 * (2 * d_k * d_k * seq_len))\n",
    "    rope_flops += num_heads * (2 * (2 * d_k * d_k * seq_len))\n",
    "\n",
    "    # scaled_dot_product_attention 1 for num_heads: \"... seq d_k, ... seq d_k -> ... seq seq\"\n",
    "    num_matmul_flops += num_heads * (2 * seq_len * d_k * seq_len)\n",
    "\n",
    "    # scaled_dot_product_attention 2 for num_heads: \"... seq seq, ... seq d_k -> ... seq d_k\"\n",
    "    num_matmul_flops += num_heads * (2 * seq_len * seq_len * d_k)\n",
    "\n",
    "    # MHA output projection: \"... batch seq h_d_k, d_model h_d_k-> ... batch seq d_model\"\n",
    "    num_matmul_flops += 2 * seq_len * (num_heads*d_k) * d_model\n",
    "\n",
    "    mha_rope_flops += num_matmul_flops - checkpoint2_num_matmul_flops\n",
    "\n",
    "    # RMSNorm 2\n",
    "    num_matmul_flops += 0\n",
    "\n",
    "    # PositionwiseFeedforward: \n",
    "    # \"batch seq d_model, d_ff d_model -> batch seq d_ff\"\n",
    "    # \"batch seq d_model, d_ff  d_model -> batch seq d_ff\"\n",
    "    # \"batch seq d_ff, d_model  d_ff -> batch seq d_model\"\n",
    "    num_matmul_flops += 3 * (2 * seq_len * d_model * d_ff)\n",
    "    pw_feed_forward_flops += 3 * (2 * seq_len * d_model * d_ff)\n",
    "\n",
    "    ###### END Tranformer block ######\n",
    "    attn_block_flops += num_matmul_flops - checkpoint1_num_matmul_flops\n",
    "    \n",
    "# Last RMSNorm \"batch seq d_ff -> batch seq d_ff\"\n",
    "num_matmul_flops += 0\n",
    "\n",
    "# Linear projection: \"batch seq d_ff, vocab_size d_ff-> batch seq vocab_size\"\n",
    "num_matmul_flops += 2 * seq_len * d_model * vocab_size\n",
    "output_embedding_flops += 2 * seq_len * d_model * vocab_size\n",
    "# Softmax\n",
    "num_matmul_flops += 0\n",
    "\n",
    "print(f'\\\\noindent Total FLOPs: {\"{:e}\".format(num_matmul_flops).format()} \\\\\\\\')\n",
    "print()\n",
    "print(f'\\\\noindent Proportion of FLOPs in Attention Blocks: {round(100*attn_block_flops/num_matmul_flops, 2)}\\\\% \\\\\\\\')\n",
    "print(f'\\\\noindent Proportion of FLOPs for Output Embedding: {round(100*output_embedding_flops/num_matmul_flops, 2)}\\\\% \\\\\\\\')\n",
    "print()\n",
    "print(f'\\\\noindent Proportion of FLOPs for RoPE: {round(100*rope_flops/attn_block_flops, 2)}\\\\% \\\\\\\\')\n",
    "print(f'\\\\noindent Proportion of FLOPs for Multi-Head Attention with RoPE: {round(100*mha_rope_flops/attn_block_flops, 2)}\\\\% \\\\\\\\')\n",
    "print(f'\\\\noindent Proportion of FLOPs for Position-Wise Feed-Forward: {round(100*pw_feed_forward_flops/attn_block_flops, 2)}\\\\% \\\\\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc996c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.49844918272"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "149844918272000/1e14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ef0c7",
   "metadata": {},
   "source": [
    "## Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23a7bbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\noindent Total trainable parameters: 2.127058e+09 \\\\\n"
     ]
    }
   ],
   "source": [
    "num_parameters = 0\n",
    "# Embedding\n",
    "num_parameters += vocab_size * d_model\n",
    "\n",
    "for i in range(num_layers):\n",
    "    ###### START Tranformer block ######\n",
    "\n",
    "    # RMSNorm 1\n",
    "    num_parameters += d_model\n",
    "\n",
    "    # MHA with RoPE\n",
    "    # 3 matric multiplies for Q, K, V: \"... batch seq d_model, h_d_k d_model -> ... batch seq h_d_k\"\n",
    "    num_parameters += num_heads * (d_model / num_heads) * d_model\n",
    "    num_parameters += num_heads * (d_model / num_heads) * d_model\n",
    "    num_parameters += num_heads * (d_model / num_heads) * d_model\n",
    "\n",
    "    # RoPE for Q and K for num_heads: \"... seq d_k1 d_k2, ... seq d_k2 -> ... seq d_k1\"\n",
    "    num_parameters += 0\n",
    "\n",
    "    # MHA output projection: \"... batch seq h_d_k, d_model h_d_k-> ... batch seq d_model\"\n",
    "    num_parameters += (d_model / num_heads) * num_heads * d_model\n",
    "\n",
    "    # RMSNorm 2\n",
    "    num_parameters += d_model\n",
    "\n",
    "    # PositionwiseFeedforward: \n",
    "    # \"batch seq d_model, d_ff d_model -> batch seq d_ff\"\n",
    "    # \"batch seq d_model, d_ff  d_model -> batch seq d_ff\"\n",
    "    # \"batch seq d_ff, d_model  d_ff -> batch seq d_model\"\n",
    "    num_parameters += d_model * d_ff\n",
    "    num_parameters += d_ff * d_model\n",
    "    num_parameters += d_model * d_ff\n",
    "\n",
    "    ###### END Tranformer block ######\n",
    "    \n",
    "# Last RMSNorm \"batch seq d_model -> batch seq d_model\"\n",
    "num_parameters += d_model\n",
    "\n",
    "# Linear projection: \"batch seq d_model, vocab_size d_model-> batch seq vocab_size\"\n",
    "num_parameters += vocab_size * d_model\n",
    "\n",
    "# Softmax\n",
    "num_parameters += 0\n",
    "print(f'\\\\noindent Total trainable parameters: {\"{:e}\".format(num_parameters).format()} \\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9bffb400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.923907041549683"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(num_parameters * 4) / 2**30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f61e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attrib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
